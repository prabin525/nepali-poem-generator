{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"poems.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'प्रकृतिको झापट\\n - इन्दिरा चापागाईं\\nहिजाे मान्छे\\nसबथाेक छाेडेर पैसाकाे लागि दाैडिन्थ्याे\\nकर्म, कुकर्म जे सुकै गरी पैसा थुपार्थ्र्याे\\nउसलाई लाग्थ्याे कि पैसा भए पछि सबथाेक हुन्छ\\nन त साेचेकाे थियाे बहत्तर बैशाख बाह्रकाे भुकम्प आउंछ भनेर\\nन त परिकल्पना गरेकाे थियाे काेराेनाले विश्व महामारी ल्याउँछ भनेर\\nतर, आज मान्छे\\nपैसाले जीवन किन्न सक्दाेरहेनछ भन्ने बिस्तारै बुझ्दैछ\\nप्रकृतिकाे हिसाब किताब पैसाद्वारा हुँदाेरहेनछ भन्ने सुझ्दैछ\\nभूकम्पले समानता ल्याएकाे थियाे धनि गरिव दुवैलाइ पालमुनि सुताएर\\nकाेराेनाले थप पाठ पढाइरहेछ \\nधनी गरिब दुवैलाइ एकसमान मृत्यू दिएर\\nमृत्यूलाइ जित्न सकिदैन या जुनसुकै बेला जीवन किन्न सकिदैन भनेर\\nत्यसैले त मान्छे आाज फेरि एकपटक \"मानवता\" फलाक्न थालेकाे छ\\nअरुबेला विश्वासै नलाग्ने \"भगवान\" हरपल मनमनै अलाप्न थालेकाे छ\\nमृत्यूकाे त्रासले छाेपे पछि भाइचाराकाे पाठ पढ्न थालेकाे छ\\nअहमताकाे पर्दा हटाएर सबैकाे अस्तित्व हेर्न थालेकाे छ\\n\"बहुजन हिताय बहुजन सुखाय\"काे सानाे दीप हृदयमा बालेकाे छ\\nआफनाे मृत्यू धेरै टाढा छैन भन्ने कुरा मस्तिष्कमा राख्न थालेकाे छ\\nमान्छेलाई अब थाेरै भए पनि \"चेतना भया\"\\nकिन कि प्रकृतिले पटक पटक दह्राे झापट हानेकाे छ ।\\nकिन कि प्रकृतिले पटक पटक दह्राे झापट हानेकाे छ ।।\\n**\\nधुपू संखुवासभा।\\n\\n\\n****************************************************************************'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प्रकृतिको झापट\n",
      " - इन्दिरा चापागाईं\n",
      "हिजाे मान्छे\n",
      "सबथाेक छाेडेर पैसाकाे लागि दाैडिन्थ्याे\n",
      "कर्म, कुकर्म जे सुकै गरी पैसा थुपार्थ्र्याे\n",
      "उसलाई लाग्थ्याे कि पैसा भए पछि सबथाेक हुन्छ\n",
      "न त साेचेकाे थियाे बहत्तर बैशाख बाह्रकाे भुकम्प आउंछ भनेर\n",
      "न त परिकल्पना गरेकाे थियाे काेराेनाले विश्व महामारी ल्याउँछ भनेर\n",
      "तर, आज मान्छे\n",
      "पैसाले जीवन किन्न सक्दाेरहेनछ भन्ने बिस्तारै बुझ्दैछ\n",
      "प्रकृतिकाे हिसाब किताब पैसाद्वारा हुँदाेरहेनछ भन्ने सुझ्दैछ\n",
      "भूकम्पले समानता ल्याएकाे थियाे धनि गरिव दुवैलाइ पालमुनि सुताएर\n",
      "काेराेनाले थप पाठ पढाइरहेछ \n",
      "धनी गरिब दुवैलाइ एकसमान मृत्यू दिएर\n",
      "मृत्यूलाइ जित्न सकिदैन या जुनसुकै बेला जीवन किन्न सकिदैन भनेर\n",
      "त्यसैले त मान्छे आाज फेरि एकपटक \"मानवता\" फलाक्न थालेकाे छ\n",
      "अरुबेला विश्वासै नलाग्ने \"भगवान\" हरपल मनमनै अलाप्न थालेकाे छ\n",
      "मृत्यूकाे त्रासले छाेपे पछि भाइचाराकाे पाठ पढ्न थालेकाे छ\n",
      "अहमताकाे पर्दा हटाएर सबैकाे अस्तित्व हेर्न थालेकाे छ\n",
      "\"बहुजन हिताय बहुजन सुखाय\"काे सानाे दीप हृदयमा बालेकाे छ\n",
      "आफनाे मृत्यू धेरै टाढा छैन भन्ने कुरा मस्तिष्कमा राख्न थालेकाे छ\n",
      "मान्छेलाई अब थाेरै भए पनि \"चेतना भया\"\n",
      "किन कि प्रकृतिले पटक पटक दह्राे झापट हानेकाे छ ।\n",
      "किन कि प्रकृतिले पटक पटक दह्राे झापट हानेकाे छ ।।\n",
      "**\n",
      "धुपू संखुवासभा।\n",
      "\n",
      "\n",
      "****************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(text[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054830"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '%',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'H',\n",
       " 'I',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " '[',\n",
       " ']',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " '|',\n",
       " '~',\n",
       " '\\x81',\n",
       " '\\x97',\n",
       " '\\xa0',\n",
       " '¤',\n",
       " '¥',\n",
       " '¨',\n",
       " '«',\n",
       " '\\xad',\n",
       " '°',\n",
       " '²',\n",
       " '´',\n",
       " '·',\n",
       " 'º',\n",
       " 'Ç',\n",
       " 'Ó',\n",
       " 'Ô',\n",
       " '×',\n",
       " 'Þ',\n",
       " 'è',\n",
       " 'ó',\n",
       " '÷',\n",
       " 'π',\n",
       " 'ь',\n",
       " 'ࣿ',\n",
       " 'ँ',\n",
       " 'ं',\n",
       " 'ः',\n",
       " 'अ',\n",
       " 'आ',\n",
       " 'इ',\n",
       " 'ई',\n",
       " 'उ',\n",
       " 'ऊ',\n",
       " 'ऋ',\n",
       " 'ऎ',\n",
       " 'ए',\n",
       " 'ऐ',\n",
       " 'ऑ',\n",
       " 'ओ',\n",
       " 'औ',\n",
       " 'क',\n",
       " 'ख',\n",
       " 'ग',\n",
       " 'घ',\n",
       " 'ङ',\n",
       " 'च',\n",
       " 'छ',\n",
       " 'ज',\n",
       " 'झ',\n",
       " 'ञ',\n",
       " 'ट',\n",
       " 'ठ',\n",
       " 'ड',\n",
       " 'ढ',\n",
       " 'ण',\n",
       " 'त',\n",
       " 'थ',\n",
       " 'द',\n",
       " 'ध',\n",
       " 'न',\n",
       " 'ऩ',\n",
       " 'प',\n",
       " 'फ',\n",
       " 'ब',\n",
       " 'भ',\n",
       " 'म',\n",
       " 'य',\n",
       " 'र',\n",
       " 'ऱ',\n",
       " 'ल',\n",
       " 'ळ',\n",
       " 'व',\n",
       " 'श',\n",
       " 'ष',\n",
       " 'स',\n",
       " 'ह',\n",
       " '़',\n",
       " 'ऽ',\n",
       " 'ा',\n",
       " 'ि',\n",
       " 'ी',\n",
       " 'ु',\n",
       " 'ू',\n",
       " 'ृ',\n",
       " 'ॅ',\n",
       " 'ॆ',\n",
       " 'े',\n",
       " 'ै',\n",
       " 'ॉ',\n",
       " 'ॊ',\n",
       " 'ो',\n",
       " 'ौ',\n",
       " '्',\n",
       " 'ॐ',\n",
       " 'ज़',\n",
       " 'ड़',\n",
       " 'ढ़',\n",
       " 'फ़',\n",
       " 'ॠ',\n",
       " '।',\n",
       " '॥',\n",
       " '०',\n",
       " '१',\n",
       " '२',\n",
       " '३',\n",
       " '४',\n",
       " '५',\n",
       " '६',\n",
       " '७',\n",
       " '८',\n",
       " '९',\n",
       " '॰',\n",
       " 'ॲ',\n",
       " '\\u2003',\n",
       " '\\u200a',\n",
       " '\\u200b',\n",
       " '\\u200c',\n",
       " '\\u200d',\n",
       " '–',\n",
       " '—',\n",
       " '―',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '†',\n",
       " '•',\n",
       " '…',\n",
       " '⁄',\n",
       " '−',\n",
       " '≈',\n",
       " '☯',\n",
       " '☻',\n",
       " '☼',\n",
       " '\\u3000',\n",
       " '\\uf04a',\n",
       " '\\uf06c',\n",
       " '\\uf073',\n",
       " '\\uf0ae',\n",
       " '\\uf0b4',\n",
       " '\\ufeff',\n",
       " '\\U0001e240',\n",
       " '🌷'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Num to letter\n",
    "\n",
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '६',\n",
       " 1: 'm',\n",
       " 2: 'छ',\n",
       " 3: '\\uf0ae',\n",
       " 4: 'u',\n",
       " 5: '2',\n",
       " 6: 'P',\n",
       " 7: '☻',\n",
       " 8: '\\x97',\n",
       " 9: 'त',\n",
       " 10: '\\u200d',\n",
       " 11: 'A',\n",
       " 12: 'फ़',\n",
       " 13: '८',\n",
       " 14: '\\uf06c',\n",
       " 15: '=',\n",
       " 16: 'V',\n",
       " 17: '~',\n",
       " 18: 'W',\n",
       " 19: 'ó',\n",
       " 20: 'ऊ',\n",
       " 21: \"'\",\n",
       " 22: '\"',\n",
       " 23: 'I',\n",
       " 24: 'ड',\n",
       " 25: 'ड़',\n",
       " 26: 'क',\n",
       " 27: ')',\n",
       " 28: '¤',\n",
       " 29: 'ो',\n",
       " 30: '7',\n",
       " 31: '•',\n",
       " 32: '\\x81',\n",
       " 33: '—',\n",
       " 34: '‘',\n",
       " 35: ':',\n",
       " 36: '१',\n",
       " 37: 'è',\n",
       " 38: '−',\n",
       " 39: '@',\n",
       " 40: 'C',\n",
       " 41: '॰',\n",
       " 42: '―',\n",
       " 43: 'ि',\n",
       " 44: 'x',\n",
       " 45: 'D',\n",
       " 46: 'ऎ',\n",
       " 47: '%',\n",
       " 48: '–',\n",
       " 49: 'ओ',\n",
       " 50: 'ॅ',\n",
       " 51: 'Ô',\n",
       " 52: '☼',\n",
       " 53: '4',\n",
       " 54: 'य',\n",
       " 55: '\\uf04a',\n",
       " 56: 'E',\n",
       " 57: 'फ',\n",
       " 58: 'ख',\n",
       " 59: '*',\n",
       " 60: 'r',\n",
       " 61: '²',\n",
       " 62: 'Ç',\n",
       " 63: '४',\n",
       " 64: 'ठ',\n",
       " 65: 'g',\n",
       " 66: '☯',\n",
       " 67: 'ू',\n",
       " 68: '/',\n",
       " 69: 'ु',\n",
       " 70: 'F',\n",
       " 71: 'ः',\n",
       " 72: 'i',\n",
       " 73: '२',\n",
       " 74: 'ढ',\n",
       " 75: '5',\n",
       " 76: '\\u3000',\n",
       " 77: 'ञ',\n",
       " 78: '़',\n",
       " 79: 'ल',\n",
       " 80: 'ढ़',\n",
       " 81: '\\u200c',\n",
       " 82: 'ॠ',\n",
       " 83: 'द',\n",
       " 84: '¥',\n",
       " 85: 'े',\n",
       " 86: 'न',\n",
       " 87: '¨',\n",
       " 88: '🌷',\n",
       " 89: '\\n',\n",
       " 90: '1',\n",
       " 91: 'R',\n",
       " 92: 'o',\n",
       " 93: '°',\n",
       " 94: 'j',\n",
       " 95: 'ग',\n",
       " 96: 'c',\n",
       " 97: '०',\n",
       " 98: '`',\n",
       " 99: '\\u200b',\n",
       " 100: '×',\n",
       " 101: 'ृ',\n",
       " 102: 'ई',\n",
       " 103: 'U',\n",
       " 104: '«',\n",
       " 105: '-',\n",
       " 106: 'ऑ',\n",
       " 107: 'र',\n",
       " 108: '+',\n",
       " 109: ' ',\n",
       " 110: '⁄',\n",
       " 111: '>',\n",
       " 112: 'आ',\n",
       " 113: ';',\n",
       " 114: 'K',\n",
       " 115: '७',\n",
       " 116: 'Þ',\n",
       " 117: '≈',\n",
       " 118: 'y',\n",
       " 119: 's',\n",
       " 120: 'T',\n",
       " 121: 'ऋ',\n",
       " 122: 'अ',\n",
       " 123: 'उ',\n",
       " 124: 'h',\n",
       " 125: 'v',\n",
       " 126: 'ी',\n",
       " 127: '\\uf0b4',\n",
       " 128: 'ह',\n",
       " 129: 'घ',\n",
       " 130: 'ॲ',\n",
       " 131: 'ॊ',\n",
       " 132: 'म',\n",
       " 133: 'ऽ',\n",
       " 134: '्',\n",
       " 135: '५',\n",
       " 136: 'प',\n",
       " 137: 'd',\n",
       " 138: 'व',\n",
       " 139: 'S',\n",
       " 140: 'n',\n",
       " 141: 'औ',\n",
       " 142: 'Ó',\n",
       " 143: 'ळ',\n",
       " 144: 'ष',\n",
       " 145: '।',\n",
       " 146: 'l',\n",
       " 147: '†',\n",
       " 148: 'श',\n",
       " 149: 'ऐ',\n",
       " 150: '\\U0001e240',\n",
       " 151: 'º',\n",
       " 152: '9',\n",
       " 153: 'झ',\n",
       " 154: 'ь',\n",
       " 155: '|',\n",
       " 156: 't',\n",
       " 157: 'e',\n",
       " 158: '“',\n",
       " 159: 'ॉ',\n",
       " 160: 'w',\n",
       " 161: '”',\n",
       " 162: '6',\n",
       " 163: 'इ',\n",
       " 164: 'B',\n",
       " 165: 'H',\n",
       " 166: '\\xa0',\n",
       " 167: '॥',\n",
       " 168: 'च',\n",
       " 169: '\\u2003',\n",
       " 170: 'b',\n",
       " 171: '!',\n",
       " 172: '0',\n",
       " 173: 'ॆ',\n",
       " 174: 'f',\n",
       " 175: '\\xad',\n",
       " 176: '#',\n",
       " 177: '३',\n",
       " 178: '\\u200a',\n",
       " 179: '\\ufeff',\n",
       " 180: '.',\n",
       " 181: 'M',\n",
       " 182: 'ँ',\n",
       " 183: 'ौ',\n",
       " 184: '÷',\n",
       " 185: 'O',\n",
       " 186: 'ए',\n",
       " 187: '’',\n",
       " 188: '९',\n",
       " 189: 'ङ',\n",
       " 190: 'a',\n",
       " 191: '3',\n",
       " 192: '\\t',\n",
       " 193: 'N',\n",
       " 194: 'थ',\n",
       " 195: 'π',\n",
       " 196: 'ै',\n",
       " 197: 'ण',\n",
       " 198: 'भ',\n",
       " 199: 'L',\n",
       " 200: 'ज',\n",
       " 201: '\\uf073',\n",
       " 202: '8',\n",
       " 203: 'ऩ',\n",
       " 204: '(',\n",
       " 205: 'स',\n",
       " 206: 'k',\n",
       " 207: 'ध',\n",
       " 208: '?',\n",
       " 209: 'X',\n",
       " 210: 'ट',\n",
       " 211: 'ऱ',\n",
       " 212: '·',\n",
       " 213: ',',\n",
       " 214: 'ज़',\n",
       " 215: 'p',\n",
       " 216: 'ࣿ',\n",
       " 217: '[',\n",
       " 218: '…',\n",
       " 219: '´',\n",
       " 220: ']',\n",
       " 221: 'ा',\n",
       " 222: 'ं',\n",
       " 223: 'ब',\n",
       " 224: 'ॐ'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder :  Letter to num\n",
    "\n",
    "encoder = {v:k for k,v in decoder.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'६': 0,\n",
       " 'm': 1,\n",
       " 'छ': 2,\n",
       " '\\uf0ae': 3,\n",
       " 'u': 4,\n",
       " '2': 5,\n",
       " 'P': 6,\n",
       " '☻': 7,\n",
       " '\\x97': 8,\n",
       " 'त': 9,\n",
       " '\\u200d': 10,\n",
       " 'A': 11,\n",
       " 'फ़': 12,\n",
       " '८': 13,\n",
       " '\\uf06c': 14,\n",
       " '=': 15,\n",
       " 'V': 16,\n",
       " '~': 17,\n",
       " 'W': 18,\n",
       " 'ó': 19,\n",
       " 'ऊ': 20,\n",
       " \"'\": 21,\n",
       " '\"': 22,\n",
       " 'I': 23,\n",
       " 'ड': 24,\n",
       " 'ड़': 25,\n",
       " 'क': 26,\n",
       " ')': 27,\n",
       " '¤': 28,\n",
       " 'ो': 29,\n",
       " '7': 30,\n",
       " '•': 31,\n",
       " '\\x81': 32,\n",
       " '—': 33,\n",
       " '‘': 34,\n",
       " ':': 35,\n",
       " '१': 36,\n",
       " 'è': 37,\n",
       " '−': 38,\n",
       " '@': 39,\n",
       " 'C': 40,\n",
       " '॰': 41,\n",
       " '―': 42,\n",
       " 'ि': 43,\n",
       " 'x': 44,\n",
       " 'D': 45,\n",
       " 'ऎ': 46,\n",
       " '%': 47,\n",
       " '–': 48,\n",
       " 'ओ': 49,\n",
       " 'ॅ': 50,\n",
       " 'Ô': 51,\n",
       " '☼': 52,\n",
       " '4': 53,\n",
       " 'य': 54,\n",
       " '\\uf04a': 55,\n",
       " 'E': 56,\n",
       " 'फ': 57,\n",
       " 'ख': 58,\n",
       " '*': 59,\n",
       " 'r': 60,\n",
       " '²': 61,\n",
       " 'Ç': 62,\n",
       " '४': 63,\n",
       " 'ठ': 64,\n",
       " 'g': 65,\n",
       " '☯': 66,\n",
       " 'ू': 67,\n",
       " '/': 68,\n",
       " 'ु': 69,\n",
       " 'F': 70,\n",
       " 'ः': 71,\n",
       " 'i': 72,\n",
       " '२': 73,\n",
       " 'ढ': 74,\n",
       " '5': 75,\n",
       " '\\u3000': 76,\n",
       " 'ञ': 77,\n",
       " '़': 78,\n",
       " 'ल': 79,\n",
       " 'ढ़': 80,\n",
       " '\\u200c': 81,\n",
       " 'ॠ': 82,\n",
       " 'द': 83,\n",
       " '¥': 84,\n",
       " 'े': 85,\n",
       " 'न': 86,\n",
       " '¨': 87,\n",
       " '🌷': 88,\n",
       " '\\n': 89,\n",
       " '1': 90,\n",
       " 'R': 91,\n",
       " 'o': 92,\n",
       " '°': 93,\n",
       " 'j': 94,\n",
       " 'ग': 95,\n",
       " 'c': 96,\n",
       " '०': 97,\n",
       " '`': 98,\n",
       " '\\u200b': 99,\n",
       " '×': 100,\n",
       " 'ृ': 101,\n",
       " 'ई': 102,\n",
       " 'U': 103,\n",
       " '«': 104,\n",
       " '-': 105,\n",
       " 'ऑ': 106,\n",
       " 'र': 107,\n",
       " '+': 108,\n",
       " ' ': 109,\n",
       " '⁄': 110,\n",
       " '>': 111,\n",
       " 'आ': 112,\n",
       " ';': 113,\n",
       " 'K': 114,\n",
       " '७': 115,\n",
       " 'Þ': 116,\n",
       " '≈': 117,\n",
       " 'y': 118,\n",
       " 's': 119,\n",
       " 'T': 120,\n",
       " 'ऋ': 121,\n",
       " 'अ': 122,\n",
       " 'उ': 123,\n",
       " 'h': 124,\n",
       " 'v': 125,\n",
       " 'ी': 126,\n",
       " '\\uf0b4': 127,\n",
       " 'ह': 128,\n",
       " 'घ': 129,\n",
       " 'ॲ': 130,\n",
       " 'ॊ': 131,\n",
       " 'म': 132,\n",
       " 'ऽ': 133,\n",
       " '्': 134,\n",
       " '५': 135,\n",
       " 'प': 136,\n",
       " 'd': 137,\n",
       " 'व': 138,\n",
       " 'S': 139,\n",
       " 'n': 140,\n",
       " 'औ': 141,\n",
       " 'Ó': 142,\n",
       " 'ळ': 143,\n",
       " 'ष': 144,\n",
       " '।': 145,\n",
       " 'l': 146,\n",
       " '†': 147,\n",
       " 'श': 148,\n",
       " 'ऐ': 149,\n",
       " '\\U0001e240': 150,\n",
       " 'º': 151,\n",
       " '9': 152,\n",
       " 'झ': 153,\n",
       " 'ь': 154,\n",
       " '|': 155,\n",
       " 't': 156,\n",
       " 'e': 157,\n",
       " '“': 158,\n",
       " 'ॉ': 159,\n",
       " 'w': 160,\n",
       " '”': 161,\n",
       " '6': 162,\n",
       " 'इ': 163,\n",
       " 'B': 164,\n",
       " 'H': 165,\n",
       " '\\xa0': 166,\n",
       " '॥': 167,\n",
       " 'च': 168,\n",
       " '\\u2003': 169,\n",
       " 'b': 170,\n",
       " '!': 171,\n",
       " '0': 172,\n",
       " 'ॆ': 173,\n",
       " 'f': 174,\n",
       " '\\xad': 175,\n",
       " '#': 176,\n",
       " '३': 177,\n",
       " '\\u200a': 178,\n",
       " '\\ufeff': 179,\n",
       " '.': 180,\n",
       " 'M': 181,\n",
       " 'ँ': 182,\n",
       " 'ौ': 183,\n",
       " '÷': 184,\n",
       " 'O': 185,\n",
       " 'ए': 186,\n",
       " '’': 187,\n",
       " '९': 188,\n",
       " 'ङ': 189,\n",
       " 'a': 190,\n",
       " '3': 191,\n",
       " '\\t': 192,\n",
       " 'N': 193,\n",
       " 'थ': 194,\n",
       " 'π': 195,\n",
       " 'ै': 196,\n",
       " 'ण': 197,\n",
       " 'भ': 198,\n",
       " 'L': 199,\n",
       " 'ज': 200,\n",
       " '\\uf073': 201,\n",
       " '8': 202,\n",
       " 'ऩ': 203,\n",
       " '(': 204,\n",
       " 'स': 205,\n",
       " 'k': 206,\n",
       " 'ध': 207,\n",
       " '?': 208,\n",
       " 'X': 209,\n",
       " 'ट': 210,\n",
       " 'ऱ': 211,\n",
       " '·': 212,\n",
       " ',': 213,\n",
       " 'ज़': 214,\n",
       " 'p': 215,\n",
       " 'ࣿ': 216,\n",
       " '[': 217,\n",
       " '…': 218,\n",
       " '´': 219,\n",
       " ']': 220,\n",
       " 'ा': 221,\n",
       " 'ं': 222,\n",
       " 'ब': 223,\n",
       " 'ॐ': 224}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([136, 134, 107,  26, 101,   9,  43,  26,  29, 109, 153, 221, 136,\n",
       "       210,  89, 109, 105, 109, 163,  86, 134,  83,  43, 107, 221, 109,\n",
       "       168, 221, 136, 221,  95, 221, 102, 222,  89, 128,  43, 200, 221,\n",
       "        85, 109, 132, 221,  86, 134,   2,  85,  89, 205, 223, 194, 221,\n",
       "        85,  26, 109,   2, 221,  85,  24,  85, 107, 109, 136, 196, 205,\n",
       "       221,  26, 221,  85, 109,  79, 221,  95,  43, 109,  83, 221, 196,\n",
       "        24,  43,  86, 134, 194, 134,  54, 221,  85,  89,  26, 107, 134,\n",
       "       132, 213, 109,  26,  69,  26, 107, 134, 132])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one hot encoder\n",
    "\n",
    "def one_hot_encoder(encoded_text, num_uni_characters):\n",
    "    \n",
    "    \n",
    "    # encoded_text --> batch of encoded text\n",
    "    # num_uni_characters --> len(set(text))\n",
    "    \n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_characters))\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_characters))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape(4,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, dropout_porbability=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_porbability = dropout_porbability\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:ind for ind, char in decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            len(self.all_chars),\n",
    "            num_hidden,\n",
    "            num_layers,\n",
    "            dropout=dropout_porbability,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_porbability)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "    \n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda()\n",
    "            )\n",
    "        else:\n",
    "            hidden = (\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden)\n",
    "            )\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    dropout_porbability=0.5,\n",
    "    use_gpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1849347"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205483"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "tracker = 0\n",
    "\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 3.4744036197662354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3a90af4e75e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# POSSIBLE EXPLODING GRADIENT PROBLEM!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nepali_poem.net'\n",
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(225, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=225, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    dropout_porbability=0.5,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "    \n",
    "    encoded_text = model.encoder[char]\n",
    "    \n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    \n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "    \n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    \n",
    "    lstm_out , hidden = model(inputs, hidden)\n",
    "    \n",
    "    probs = F.softmax(lstm_out, dim=1).data\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        probs = probs.cpu()\n",
    "        \n",
    "    probs, index_positions = probs.topk(k)\n",
    "    \n",
    "    index_positions = index_positions.numpy().squeeze()\n",
    "    \n",
    "    probs = probs.numpy().flatten()\n",
    "    \n",
    "    probs = probs/probs.sum()\n",
    "    \n",
    "    char = np.random.choice(index_positions, p=probs)\n",
    "    \n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed=\"The\", k=1):\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    output_chars.append(char)\n",
    "    \n",
    "    for i in range(size):\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        output_chars.append(char)\n",
    "    \n",
    "    \n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मेरो ा ा ्     ा    ा ा्  ा्् ा  ्ाा     ् ा  ा ा््    ्  ा ्््  ा ्ाा् ्ा  ााा  ाा ्ा्ा  ्   ा््ा्ा्  ााा ््ाा  ्  ा् ा     ्् ा   ा्  ाा    ्ा्ाााा  ाा्   ा ा     ा््ा  ् ााा्   ा ्         ्ा  ा्् ्  ाा  ाा ्ा ा ाा् ा  ा ा््  ा ्ा  ्ाा ा्      ा    ाा ा ्ाााा  ्ाा्ा   ाा  ा ्ा्ा ााा  ्ाा  ्ा ा ा््  ् ाााा ्् ्ा    ा          ् ््् ााा ्ाा  ा् ा   ्     ाा ्  ् ाा   ््  ््  ाा ्ााा् ््   ्ा ाा ा ्ाा   ा् ा ा्  ्ा ्ााा ् ्ा्् ा   ् ाा्््  ा ाा ् ाा ा्  ाा  ा  ा    ्   ्  ाा्ाा   ्  ा    ा    ाा ा्   ्  ् ा    ््ा ा    ाा्  ् ् ा ा् ्    ा ाा ााा् ्ाा् ा ा  ्ा  ््ााा   ्ा् ्ाा  ाा  ा् ्ााा ा    ्ा ा् ा  ्  ा  ा्ाााा् ा् ्  ा्   ा      ा््ा ा्  ््् ा ा ा     ्ा      ््   ा््  ्  ा  ्  ा ् ् ा ्् ््         ा्  ा       ाा  ा्् ्  ा  ा  ्ा्  ्  ्   ाा  ा ााा     ् ्ाााा्  ् ा््ा  ्ा       ् ा    ्ा््ा     ्ााा् ा ्ाा ् ा     ा ्ा  ा््    ा  ्  ा ् ् ा ््ा्    ्ा    ा    ा  ्ा ्ााा्    ्ा ा  ााा््् ा   ््  ्    ्  ्  ा्ा्    ा  ्ाा ्््   ्ा ा     ाा    ा््ा  ााा्ा  ा्  ्ा्   ्् ् ् ््््््ाा ा ््\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed = \"मेरो \", k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
