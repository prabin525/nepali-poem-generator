{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"poems.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§ï‡•ã ‡§ù‡§æ‡§™‡§ü\\n - ‡§á‡§®‡•ç‡§¶‡§ø‡§∞‡§æ ‡§ö‡§æ‡§™‡§æ‡§ó‡§æ‡§à‡§Ç\\n‡§π‡§ø‡§ú‡§æ‡•á ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á\\n‡§∏‡§¨‡§•‡§æ‡•á‡§ï ‡§õ‡§æ‡•á‡§°‡•á‡§∞ ‡§™‡•à‡§∏‡§æ‡§ï‡§æ‡•á ‡§≤‡§æ‡§ó‡§ø ‡§¶‡§æ‡•à‡§°‡§ø‡§®‡•ç‡§•‡•ç‡§Ø‡§æ‡•á\\n‡§ï‡§∞‡•ç‡§Æ, ‡§ï‡•Å‡§ï‡§∞‡•ç‡§Æ ‡§ú‡•á ‡§∏‡•Å‡§ï‡•à ‡§ó‡§∞‡•Ä ‡§™‡•à‡§∏‡§æ ‡§•‡•Å‡§™‡§æ‡§∞‡•ç‡§•‡•ç‡§∞‡•ç‡§Ø‡§æ‡•á\\n‡§â‡§∏‡§≤‡§æ‡§à ‡§≤‡§æ‡§ó‡•ç‡§•‡•ç‡§Ø‡§æ‡•á ‡§ï‡§ø ‡§™‡•à‡§∏‡§æ ‡§≠‡§è ‡§™‡§õ‡§ø ‡§∏‡§¨‡§•‡§æ‡•á‡§ï ‡§π‡•Å‡§®‡•ç‡§õ\\n‡§® ‡§§ ‡§∏‡§æ‡•á‡§ö‡•á‡§ï‡§æ‡•á ‡§•‡§ø‡§Ø‡§æ‡•á ‡§¨‡§π‡§§‡•ç‡§§‡§∞ ‡§¨‡•à‡§∂‡§æ‡§ñ ‡§¨‡§æ‡§π‡•ç‡§∞‡§ï‡§æ‡•á ‡§≠‡•Å‡§ï‡§Æ‡•ç‡§™ ‡§Ü‡§â‡§Ç‡§õ ‡§≠‡§®‡•á‡§∞\\n‡§® ‡§§ ‡§™‡§∞‡§ø‡§ï‡§≤‡•ç‡§™‡§®‡§æ ‡§ó‡§∞‡•á‡§ï‡§æ‡•á ‡§•‡§ø‡§Ø‡§æ‡•á ‡§ï‡§æ‡•á‡§∞‡§æ‡•á‡§®‡§æ‡§≤‡•á ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä ‡§≤‡•ç‡§Ø‡§æ‡§â‡§Å‡§õ ‡§≠‡§®‡•á‡§∞\\n‡§§‡§∞, ‡§Ü‡§ú ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á\\n‡§™‡•à‡§∏‡§æ‡§≤‡•á ‡§ú‡•Ä‡§µ‡§® ‡§ï‡§ø‡§®‡•ç‡§® ‡§∏‡§ï‡•ç‡§¶‡§æ‡•á‡§∞‡§π‡•á‡§®‡§õ ‡§≠‡§®‡•ç‡§®‡•á ‡§¨‡§ø‡§∏‡•ç‡§§‡§æ‡§∞‡•à ‡§¨‡•Å‡§ù‡•ç‡§¶‡•à‡§õ\\n‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§ï‡§æ‡•á ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§™‡•à‡§∏‡§æ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§π‡•Å‡§Å‡§¶‡§æ‡•á‡§∞‡§π‡•á‡§®‡§õ ‡§≠‡§®‡•ç‡§®‡•á ‡§∏‡•Å‡§ù‡•ç‡§¶‡•à‡§õ\\n‡§≠‡•Ç‡§ï‡§Æ‡•ç‡§™‡§≤‡•á ‡§∏‡§Æ‡§æ‡§®‡§§‡§æ ‡§≤‡•ç‡§Ø‡§æ‡§è‡§ï‡§æ‡•á ‡§•‡§ø‡§Ø‡§æ‡•á ‡§ß‡§®‡§ø ‡§ó‡§∞‡§ø‡§µ ‡§¶‡•Å‡§µ‡•à‡§≤‡§æ‡§á ‡§™‡§æ‡§≤‡§Æ‡•Å‡§®‡§ø ‡§∏‡•Å‡§§‡§æ‡§è‡§∞\\n‡§ï‡§æ‡•á‡§∞‡§æ‡•á‡§®‡§æ‡§≤‡•á ‡§•‡§™ ‡§™‡§æ‡§† ‡§™‡§¢‡§æ‡§á‡§∞‡§π‡•á‡§õ \\n‡§ß‡§®‡•Ä ‡§ó‡§∞‡§ø‡§¨ ‡§¶‡•Å‡§µ‡•à‡§≤‡§æ‡§á ‡§è‡§ï‡§∏‡§Æ‡§æ‡§® ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç ‡§¶‡§ø‡§è‡§∞\\n‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç‡§≤‡§æ‡§á ‡§ú‡§ø‡§§‡•ç‡§® ‡§∏‡§ï‡§ø‡§¶‡•à‡§® ‡§Ø‡§æ ‡§ú‡•Å‡§®‡§∏‡•Å‡§ï‡•à ‡§¨‡•á‡§≤‡§æ ‡§ú‡•Ä‡§µ‡§® ‡§ï‡§ø‡§®‡•ç‡§® ‡§∏‡§ï‡§ø‡§¶‡•à‡§® ‡§≠‡§®‡•á‡§∞\\n‡§§‡•ç‡§Ø‡§∏‡•à‡§≤‡•á ‡§§ ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á ‡§Ü‡§æ‡§ú ‡§´‡•á‡§∞‡§ø ‡§è‡§ï‡§™‡§ü‡§ï \"‡§Æ‡§æ‡§®‡§µ‡§§‡§æ\" ‡§´‡§≤‡§æ‡§ï‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\\n‡§Ö‡§∞‡•Å‡§¨‡•á‡§≤‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏‡•à ‡§®‡§≤‡§æ‡§ó‡•ç‡§®‡•á \"‡§≠‡§ó‡§µ‡§æ‡§®\" ‡§π‡§∞‡§™‡§≤ ‡§Æ‡§®‡§Æ‡§®‡•à ‡§Ö‡§≤‡§æ‡§™‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\\n‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç‡§ï‡§æ‡•á ‡§§‡•ç‡§∞‡§æ‡§∏‡§≤‡•á ‡§õ‡§æ‡•á‡§™‡•á ‡§™‡§õ‡§ø ‡§≠‡§æ‡§á‡§ö‡§æ‡§∞‡§æ‡§ï‡§æ‡•á ‡§™‡§æ‡§† ‡§™‡§¢‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\\n‡§Ö‡§π‡§Æ‡§§‡§æ‡§ï‡§æ‡•á ‡§™‡§∞‡•ç‡§¶‡§æ ‡§π‡§ü‡§æ‡§è‡§∞ ‡§∏‡§¨‡•à‡§ï‡§æ‡•á ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç‡§µ ‡§π‡•á‡§∞‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\\n\"‡§¨‡§π‡•Å‡§ú‡§® ‡§π‡§ø‡§§‡§æ‡§Ø ‡§¨‡§π‡•Å‡§ú‡§® ‡§∏‡•Å‡§ñ‡§æ‡§Ø\"‡§ï‡§æ‡•á ‡§∏‡§æ‡§®‡§æ‡•á ‡§¶‡•Ä‡§™ ‡§π‡•É‡§¶‡§Ø‡§Æ‡§æ ‡§¨‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\\n‡§Ü‡§´‡§®‡§æ‡•á ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç ‡§ß‡•á‡§∞‡•à ‡§ü‡§æ‡§¢‡§æ ‡§õ‡•à‡§® ‡§≠‡§®‡•ç‡§®‡•á ‡§ï‡•Å‡§∞‡§æ ‡§Æ‡§∏‡•ç‡§§‡§ø‡§∑‡•ç‡§ï‡§Æ‡§æ ‡§∞‡§æ‡§ñ‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\\n‡§Æ‡§æ‡§®‡•ç‡§õ‡•á‡§≤‡§æ‡§à ‡§Ö‡§¨ ‡§•‡§æ‡•á‡§∞‡•à ‡§≠‡§è ‡§™‡§®‡§ø \"‡§ö‡•á‡§§‡§®‡§æ ‡§≠‡§Ø‡§æ\"\\n‡§ï‡§ø‡§® ‡§ï‡§ø ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§≤‡•á ‡§™‡§ü‡§ï ‡§™‡§ü‡§ï ‡§¶‡§π‡•ç‡§∞‡§æ‡•á ‡§ù‡§æ‡§™‡§ü ‡§π‡§æ‡§®‡•á‡§ï‡§æ‡•á ‡§õ ‡•§\\n‡§ï‡§ø‡§® ‡§ï‡§ø ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§≤‡•á ‡§™‡§ü‡§ï ‡§™‡§ü‡§ï ‡§¶‡§π‡•ç‡§∞‡§æ‡•á ‡§ù‡§æ‡§™‡§ü ‡§π‡§æ‡§®‡•á‡§ï‡§æ‡•á ‡§õ ‡•§‡•§\\n**\\n‡§ß‡•Å‡§™‡•Ç ‡§∏‡§Ç‡§ñ‡•Å‡§µ‡§æ‡§∏‡§≠‡§æ‡•§\\n\\n\\n****************************************************************************'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§ï‡•ã ‡§ù‡§æ‡§™‡§ü\n",
      " - ‡§á‡§®‡•ç‡§¶‡§ø‡§∞‡§æ ‡§ö‡§æ‡§™‡§æ‡§ó‡§æ‡§à‡§Ç\n",
      "‡§π‡§ø‡§ú‡§æ‡•á ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á\n",
      "‡§∏‡§¨‡§•‡§æ‡•á‡§ï ‡§õ‡§æ‡•á‡§°‡•á‡§∞ ‡§™‡•à‡§∏‡§æ‡§ï‡§æ‡•á ‡§≤‡§æ‡§ó‡§ø ‡§¶‡§æ‡•à‡§°‡§ø‡§®‡•ç‡§•‡•ç‡§Ø‡§æ‡•á\n",
      "‡§ï‡§∞‡•ç‡§Æ, ‡§ï‡•Å‡§ï‡§∞‡•ç‡§Æ ‡§ú‡•á ‡§∏‡•Å‡§ï‡•à ‡§ó‡§∞‡•Ä ‡§™‡•à‡§∏‡§æ ‡§•‡•Å‡§™‡§æ‡§∞‡•ç‡§•‡•ç‡§∞‡•ç‡§Ø‡§æ‡•á\n",
      "‡§â‡§∏‡§≤‡§æ‡§à ‡§≤‡§æ‡§ó‡•ç‡§•‡•ç‡§Ø‡§æ‡•á ‡§ï‡§ø ‡§™‡•à‡§∏‡§æ ‡§≠‡§è ‡§™‡§õ‡§ø ‡§∏‡§¨‡§•‡§æ‡•á‡§ï ‡§π‡•Å‡§®‡•ç‡§õ\n",
      "‡§® ‡§§ ‡§∏‡§æ‡•á‡§ö‡•á‡§ï‡§æ‡•á ‡§•‡§ø‡§Ø‡§æ‡•á ‡§¨‡§π‡§§‡•ç‡§§‡§∞ ‡§¨‡•à‡§∂‡§æ‡§ñ ‡§¨‡§æ‡§π‡•ç‡§∞‡§ï‡§æ‡•á ‡§≠‡•Å‡§ï‡§Æ‡•ç‡§™ ‡§Ü‡§â‡§Ç‡§õ ‡§≠‡§®‡•á‡§∞\n",
      "‡§® ‡§§ ‡§™‡§∞‡§ø‡§ï‡§≤‡•ç‡§™‡§®‡§æ ‡§ó‡§∞‡•á‡§ï‡§æ‡•á ‡§•‡§ø‡§Ø‡§æ‡•á ‡§ï‡§æ‡•á‡§∞‡§æ‡•á‡§®‡§æ‡§≤‡•á ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä ‡§≤‡•ç‡§Ø‡§æ‡§â‡§Å‡§õ ‡§≠‡§®‡•á‡§∞\n",
      "‡§§‡§∞, ‡§Ü‡§ú ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á\n",
      "‡§™‡•à‡§∏‡§æ‡§≤‡•á ‡§ú‡•Ä‡§µ‡§® ‡§ï‡§ø‡§®‡•ç‡§® ‡§∏‡§ï‡•ç‡§¶‡§æ‡•á‡§∞‡§π‡•á‡§®‡§õ ‡§≠‡§®‡•ç‡§®‡•á ‡§¨‡§ø‡§∏‡•ç‡§§‡§æ‡§∞‡•à ‡§¨‡•Å‡§ù‡•ç‡§¶‡•à‡§õ\n",
      "‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§ï‡§æ‡•á ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§™‡•à‡§∏‡§æ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§π‡•Å‡§Å‡§¶‡§æ‡•á‡§∞‡§π‡•á‡§®‡§õ ‡§≠‡§®‡•ç‡§®‡•á ‡§∏‡•Å‡§ù‡•ç‡§¶‡•à‡§õ\n",
      "‡§≠‡•Ç‡§ï‡§Æ‡•ç‡§™‡§≤‡•á ‡§∏‡§Æ‡§æ‡§®‡§§‡§æ ‡§≤‡•ç‡§Ø‡§æ‡§è‡§ï‡§æ‡•á ‡§•‡§ø‡§Ø‡§æ‡•á ‡§ß‡§®‡§ø ‡§ó‡§∞‡§ø‡§µ ‡§¶‡•Å‡§µ‡•à‡§≤‡§æ‡§á ‡§™‡§æ‡§≤‡§Æ‡•Å‡§®‡§ø ‡§∏‡•Å‡§§‡§æ‡§è‡§∞\n",
      "‡§ï‡§æ‡•á‡§∞‡§æ‡•á‡§®‡§æ‡§≤‡•á ‡§•‡§™ ‡§™‡§æ‡§† ‡§™‡§¢‡§æ‡§á‡§∞‡§π‡•á‡§õ \n",
      "‡§ß‡§®‡•Ä ‡§ó‡§∞‡§ø‡§¨ ‡§¶‡•Å‡§µ‡•à‡§≤‡§æ‡§á ‡§è‡§ï‡§∏‡§Æ‡§æ‡§® ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç ‡§¶‡§ø‡§è‡§∞\n",
      "‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç‡§≤‡§æ‡§á ‡§ú‡§ø‡§§‡•ç‡§® ‡§∏‡§ï‡§ø‡§¶‡•à‡§® ‡§Ø‡§æ ‡§ú‡•Å‡§®‡§∏‡•Å‡§ï‡•à ‡§¨‡•á‡§≤‡§æ ‡§ú‡•Ä‡§µ‡§® ‡§ï‡§ø‡§®‡•ç‡§® ‡§∏‡§ï‡§ø‡§¶‡•à‡§® ‡§≠‡§®‡•á‡§∞\n",
      "‡§§‡•ç‡§Ø‡§∏‡•à‡§≤‡•á ‡§§ ‡§Æ‡§æ‡§®‡•ç‡§õ‡•á ‡§Ü‡§æ‡§ú ‡§´‡•á‡§∞‡§ø ‡§è‡§ï‡§™‡§ü‡§ï \"‡§Æ‡§æ‡§®‡§µ‡§§‡§æ\" ‡§´‡§≤‡§æ‡§ï‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\n",
      "‡§Ö‡§∞‡•Å‡§¨‡•á‡§≤‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏‡•à ‡§®‡§≤‡§æ‡§ó‡•ç‡§®‡•á \"‡§≠‡§ó‡§µ‡§æ‡§®\" ‡§π‡§∞‡§™‡§≤ ‡§Æ‡§®‡§Æ‡§®‡•à ‡§Ö‡§≤‡§æ‡§™‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\n",
      "‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç‡§ï‡§æ‡•á ‡§§‡•ç‡§∞‡§æ‡§∏‡§≤‡•á ‡§õ‡§æ‡•á‡§™‡•á ‡§™‡§õ‡§ø ‡§≠‡§æ‡§á‡§ö‡§æ‡§∞‡§æ‡§ï‡§æ‡•á ‡§™‡§æ‡§† ‡§™‡§¢‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\n",
      "‡§Ö‡§π‡§Æ‡§§‡§æ‡§ï‡§æ‡•á ‡§™‡§∞‡•ç‡§¶‡§æ ‡§π‡§ü‡§æ‡§è‡§∞ ‡§∏‡§¨‡•à‡§ï‡§æ‡•á ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç‡§µ ‡§π‡•á‡§∞‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\n",
      "\"‡§¨‡§π‡•Å‡§ú‡§® ‡§π‡§ø‡§§‡§æ‡§Ø ‡§¨‡§π‡•Å‡§ú‡§® ‡§∏‡•Å‡§ñ‡§æ‡§Ø\"‡§ï‡§æ‡•á ‡§∏‡§æ‡§®‡§æ‡•á ‡§¶‡•Ä‡§™ ‡§π‡•É‡§¶‡§Ø‡§Æ‡§æ ‡§¨‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\n",
      "‡§Ü‡§´‡§®‡§æ‡•á ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç ‡§ß‡•á‡§∞‡•à ‡§ü‡§æ‡§¢‡§æ ‡§õ‡•à‡§® ‡§≠‡§®‡•ç‡§®‡•á ‡§ï‡•Å‡§∞‡§æ ‡§Æ‡§∏‡•ç‡§§‡§ø‡§∑‡•ç‡§ï‡§Æ‡§æ ‡§∞‡§æ‡§ñ‡•ç‡§® ‡§•‡§æ‡§≤‡•á‡§ï‡§æ‡•á ‡§õ\n",
      "‡§Æ‡§æ‡§®‡•ç‡§õ‡•á‡§≤‡§æ‡§à ‡§Ö‡§¨ ‡§•‡§æ‡•á‡§∞‡•à ‡§≠‡§è ‡§™‡§®‡§ø \"‡§ö‡•á‡§§‡§®‡§æ ‡§≠‡§Ø‡§æ\"\n",
      "‡§ï‡§ø‡§® ‡§ï‡§ø ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§≤‡•á ‡§™‡§ü‡§ï ‡§™‡§ü‡§ï ‡§¶‡§π‡•ç‡§∞‡§æ‡•á ‡§ù‡§æ‡§™‡§ü ‡§π‡§æ‡§®‡•á‡§ï‡§æ‡•á ‡§õ ‡•§\n",
      "‡§ï‡§ø‡§® ‡§ï‡§ø ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø‡§≤‡•á ‡§™‡§ü‡§ï ‡§™‡§ü‡§ï ‡§¶‡§π‡•ç‡§∞‡§æ‡•á ‡§ù‡§æ‡§™‡§ü ‡§π‡§æ‡§®‡•á‡§ï‡§æ‡•á ‡§õ ‡•§‡•§\n",
      "**\n",
      "‡§ß‡•Å‡§™‡•Ç ‡§∏‡§Ç‡§ñ‡•Å‡§µ‡§æ‡§∏‡§≠‡§æ‡•§\n",
      "\n",
      "\n",
      "****************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(text[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054830"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '%',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'H',\n",
       " 'I',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " '[',\n",
       " ']',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " '|',\n",
       " '~',\n",
       " '\\x81',\n",
       " '\\x97',\n",
       " '\\xa0',\n",
       " '¬§',\n",
       " '¬•',\n",
       " '¬®',\n",
       " '¬´',\n",
       " '\\xad',\n",
       " '¬∞',\n",
       " '¬≤',\n",
       " '¬¥',\n",
       " '¬∑',\n",
       " '¬∫',\n",
       " '√á',\n",
       " '√ì',\n",
       " '√î',\n",
       " '√ó',\n",
       " '√û',\n",
       " '√®',\n",
       " '√≥',\n",
       " '√∑',\n",
       " 'œÄ',\n",
       " '—å',\n",
       " '‡£ø',\n",
       " '‡§Å',\n",
       " '‡§Ç',\n",
       " '‡§É',\n",
       " '‡§Ö',\n",
       " '‡§Ü',\n",
       " '‡§á',\n",
       " '‡§à',\n",
       " '‡§â',\n",
       " '‡§ä',\n",
       " '‡§ã',\n",
       " '‡§é',\n",
       " '‡§è',\n",
       " '‡§ê',\n",
       " '‡§ë',\n",
       " '‡§ì',\n",
       " '‡§î',\n",
       " '‡§ï',\n",
       " '‡§ñ',\n",
       " '‡§ó',\n",
       " '‡§ò',\n",
       " '‡§ô',\n",
       " '‡§ö',\n",
       " '‡§õ',\n",
       " '‡§ú',\n",
       " '‡§ù',\n",
       " '‡§û',\n",
       " '‡§ü',\n",
       " '‡§†',\n",
       " '‡§°',\n",
       " '‡§¢',\n",
       " '‡§£',\n",
       " '‡§§',\n",
       " '‡§•',\n",
       " '‡§¶',\n",
       " '‡§ß',\n",
       " '‡§®',\n",
       " '‡§©',\n",
       " '‡§™',\n",
       " '‡§´',\n",
       " '‡§¨',\n",
       " '‡§≠',\n",
       " '‡§Æ',\n",
       " '‡§Ø',\n",
       " '‡§∞',\n",
       " '‡§±',\n",
       " '‡§≤',\n",
       " '‡§≥',\n",
       " '‡§µ',\n",
       " '‡§∂',\n",
       " '‡§∑',\n",
       " '‡§∏',\n",
       " '‡§π',\n",
       " '‡§º',\n",
       " '‡§Ω',\n",
       " '‡§æ',\n",
       " '‡§ø',\n",
       " '‡•Ä',\n",
       " '‡•Å',\n",
       " '‡•Ç',\n",
       " '‡•É',\n",
       " '‡•Ö',\n",
       " '‡•Ü',\n",
       " '‡•á',\n",
       " '‡•à',\n",
       " '‡•â',\n",
       " '‡•ä',\n",
       " '‡•ã',\n",
       " '‡•å',\n",
       " '‡•ç',\n",
       " '‡•ê',\n",
       " '‡•õ',\n",
       " '‡•ú',\n",
       " '‡•ù',\n",
       " '‡•û',\n",
       " '‡•†',\n",
       " '‡•§',\n",
       " '‡••',\n",
       " '‡•¶',\n",
       " '‡•ß',\n",
       " '‡•®',\n",
       " '‡•©',\n",
       " '‡•™',\n",
       " '‡•´',\n",
       " '‡•¨',\n",
       " '‡•≠',\n",
       " '‡•Æ',\n",
       " '‡•Ø',\n",
       " '‡•∞',\n",
       " '‡•≤',\n",
       " '\\u2003',\n",
       " '\\u200a',\n",
       " '\\u200b',\n",
       " '\\u200c',\n",
       " '\\u200d',\n",
       " '‚Äì',\n",
       " '‚Äî',\n",
       " '‚Äï',\n",
       " '‚Äò',\n",
       " '‚Äô',\n",
       " '‚Äú',\n",
       " '‚Äù',\n",
       " '‚Ä†',\n",
       " '‚Ä¢',\n",
       " '‚Ä¶',\n",
       " '‚ÅÑ',\n",
       " '‚àí',\n",
       " '‚âà',\n",
       " '‚òØ',\n",
       " '‚òª',\n",
       " '‚òº',\n",
       " '\\u3000',\n",
       " '\\uf04a',\n",
       " '\\uf06c',\n",
       " '\\uf073',\n",
       " '\\uf0ae',\n",
       " '\\uf0b4',\n",
       " '\\ufeff',\n",
       " '\\U0001e240',\n",
       " 'üå∑'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Num to letter\n",
    "\n",
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '‡•¨',\n",
       " 1: 'm',\n",
       " 2: '‡§õ',\n",
       " 3: '\\uf0ae',\n",
       " 4: 'u',\n",
       " 5: '2',\n",
       " 6: 'P',\n",
       " 7: '‚òª',\n",
       " 8: '\\x97',\n",
       " 9: '‡§§',\n",
       " 10: '\\u200d',\n",
       " 11: 'A',\n",
       " 12: '‡•û',\n",
       " 13: '‡•Æ',\n",
       " 14: '\\uf06c',\n",
       " 15: '=',\n",
       " 16: 'V',\n",
       " 17: '~',\n",
       " 18: 'W',\n",
       " 19: '√≥',\n",
       " 20: '‡§ä',\n",
       " 21: \"'\",\n",
       " 22: '\"',\n",
       " 23: 'I',\n",
       " 24: '‡§°',\n",
       " 25: '‡•ú',\n",
       " 26: '‡§ï',\n",
       " 27: ')',\n",
       " 28: '¬§',\n",
       " 29: '‡•ã',\n",
       " 30: '7',\n",
       " 31: '‚Ä¢',\n",
       " 32: '\\x81',\n",
       " 33: '‚Äî',\n",
       " 34: '‚Äò',\n",
       " 35: ':',\n",
       " 36: '‡•ß',\n",
       " 37: '√®',\n",
       " 38: '‚àí',\n",
       " 39: '@',\n",
       " 40: 'C',\n",
       " 41: '‡•∞',\n",
       " 42: '‚Äï',\n",
       " 43: '‡§ø',\n",
       " 44: 'x',\n",
       " 45: 'D',\n",
       " 46: '‡§é',\n",
       " 47: '%',\n",
       " 48: '‚Äì',\n",
       " 49: '‡§ì',\n",
       " 50: '‡•Ö',\n",
       " 51: '√î',\n",
       " 52: '‚òº',\n",
       " 53: '4',\n",
       " 54: '‡§Ø',\n",
       " 55: '\\uf04a',\n",
       " 56: 'E',\n",
       " 57: '‡§´',\n",
       " 58: '‡§ñ',\n",
       " 59: '*',\n",
       " 60: 'r',\n",
       " 61: '¬≤',\n",
       " 62: '√á',\n",
       " 63: '‡•™',\n",
       " 64: '‡§†',\n",
       " 65: 'g',\n",
       " 66: '‚òØ',\n",
       " 67: '‡•Ç',\n",
       " 68: '/',\n",
       " 69: '‡•Å',\n",
       " 70: 'F',\n",
       " 71: '‡§É',\n",
       " 72: 'i',\n",
       " 73: '‡•®',\n",
       " 74: '‡§¢',\n",
       " 75: '5',\n",
       " 76: '\\u3000',\n",
       " 77: '‡§û',\n",
       " 78: '‡§º',\n",
       " 79: '‡§≤',\n",
       " 80: '‡•ù',\n",
       " 81: '\\u200c',\n",
       " 82: '‡•†',\n",
       " 83: '‡§¶',\n",
       " 84: '¬•',\n",
       " 85: '‡•á',\n",
       " 86: '‡§®',\n",
       " 87: '¬®',\n",
       " 88: 'üå∑',\n",
       " 89: '\\n',\n",
       " 90: '1',\n",
       " 91: 'R',\n",
       " 92: 'o',\n",
       " 93: '¬∞',\n",
       " 94: 'j',\n",
       " 95: '‡§ó',\n",
       " 96: 'c',\n",
       " 97: '‡•¶',\n",
       " 98: '`',\n",
       " 99: '\\u200b',\n",
       " 100: '√ó',\n",
       " 101: '‡•É',\n",
       " 102: '‡§à',\n",
       " 103: 'U',\n",
       " 104: '¬´',\n",
       " 105: '-',\n",
       " 106: '‡§ë',\n",
       " 107: '‡§∞',\n",
       " 108: '+',\n",
       " 109: ' ',\n",
       " 110: '‚ÅÑ',\n",
       " 111: '>',\n",
       " 112: '‡§Ü',\n",
       " 113: ';',\n",
       " 114: 'K',\n",
       " 115: '‡•≠',\n",
       " 116: '√û',\n",
       " 117: '‚âà',\n",
       " 118: 'y',\n",
       " 119: 's',\n",
       " 120: 'T',\n",
       " 121: '‡§ã',\n",
       " 122: '‡§Ö',\n",
       " 123: '‡§â',\n",
       " 124: 'h',\n",
       " 125: 'v',\n",
       " 126: '‡•Ä',\n",
       " 127: '\\uf0b4',\n",
       " 128: '‡§π',\n",
       " 129: '‡§ò',\n",
       " 130: '‡•≤',\n",
       " 131: '‡•ä',\n",
       " 132: '‡§Æ',\n",
       " 133: '‡§Ω',\n",
       " 134: '‡•ç',\n",
       " 135: '‡•´',\n",
       " 136: '‡§™',\n",
       " 137: 'd',\n",
       " 138: '‡§µ',\n",
       " 139: 'S',\n",
       " 140: 'n',\n",
       " 141: '‡§î',\n",
       " 142: '√ì',\n",
       " 143: '‡§≥',\n",
       " 144: '‡§∑',\n",
       " 145: '‡•§',\n",
       " 146: 'l',\n",
       " 147: '‚Ä†',\n",
       " 148: '‡§∂',\n",
       " 149: '‡§ê',\n",
       " 150: '\\U0001e240',\n",
       " 151: '¬∫',\n",
       " 152: '9',\n",
       " 153: '‡§ù',\n",
       " 154: '—å',\n",
       " 155: '|',\n",
       " 156: 't',\n",
       " 157: 'e',\n",
       " 158: '‚Äú',\n",
       " 159: '‡•â',\n",
       " 160: 'w',\n",
       " 161: '‚Äù',\n",
       " 162: '6',\n",
       " 163: '‡§á',\n",
       " 164: 'B',\n",
       " 165: 'H',\n",
       " 166: '\\xa0',\n",
       " 167: '‡••',\n",
       " 168: '‡§ö',\n",
       " 169: '\\u2003',\n",
       " 170: 'b',\n",
       " 171: '!',\n",
       " 172: '0',\n",
       " 173: '‡•Ü',\n",
       " 174: 'f',\n",
       " 175: '\\xad',\n",
       " 176: '#',\n",
       " 177: '‡•©',\n",
       " 178: '\\u200a',\n",
       " 179: '\\ufeff',\n",
       " 180: '.',\n",
       " 181: 'M',\n",
       " 182: '‡§Å',\n",
       " 183: '‡•å',\n",
       " 184: '√∑',\n",
       " 185: 'O',\n",
       " 186: '‡§è',\n",
       " 187: '‚Äô',\n",
       " 188: '‡•Ø',\n",
       " 189: '‡§ô',\n",
       " 190: 'a',\n",
       " 191: '3',\n",
       " 192: '\\t',\n",
       " 193: 'N',\n",
       " 194: '‡§•',\n",
       " 195: 'œÄ',\n",
       " 196: '‡•à',\n",
       " 197: '‡§£',\n",
       " 198: '‡§≠',\n",
       " 199: 'L',\n",
       " 200: '‡§ú',\n",
       " 201: '\\uf073',\n",
       " 202: '8',\n",
       " 203: '‡§©',\n",
       " 204: '(',\n",
       " 205: '‡§∏',\n",
       " 206: 'k',\n",
       " 207: '‡§ß',\n",
       " 208: '?',\n",
       " 209: 'X',\n",
       " 210: '‡§ü',\n",
       " 211: '‡§±',\n",
       " 212: '¬∑',\n",
       " 213: ',',\n",
       " 214: '‡•õ',\n",
       " 215: 'p',\n",
       " 216: '‡£ø',\n",
       " 217: '[',\n",
       " 218: '‚Ä¶',\n",
       " 219: '¬¥',\n",
       " 220: ']',\n",
       " 221: '‡§æ',\n",
       " 222: '‡§Ç',\n",
       " 223: '‡§¨',\n",
       " 224: '‡•ê'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder :  Letter to num\n",
    "\n",
    "encoder = {v:k for k,v in decoder.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'‡•¨': 0,\n",
       " 'm': 1,\n",
       " '‡§õ': 2,\n",
       " '\\uf0ae': 3,\n",
       " 'u': 4,\n",
       " '2': 5,\n",
       " 'P': 6,\n",
       " '‚òª': 7,\n",
       " '\\x97': 8,\n",
       " '‡§§': 9,\n",
       " '\\u200d': 10,\n",
       " 'A': 11,\n",
       " '‡•û': 12,\n",
       " '‡•Æ': 13,\n",
       " '\\uf06c': 14,\n",
       " '=': 15,\n",
       " 'V': 16,\n",
       " '~': 17,\n",
       " 'W': 18,\n",
       " '√≥': 19,\n",
       " '‡§ä': 20,\n",
       " \"'\": 21,\n",
       " '\"': 22,\n",
       " 'I': 23,\n",
       " '‡§°': 24,\n",
       " '‡•ú': 25,\n",
       " '‡§ï': 26,\n",
       " ')': 27,\n",
       " '¬§': 28,\n",
       " '‡•ã': 29,\n",
       " '7': 30,\n",
       " '‚Ä¢': 31,\n",
       " '\\x81': 32,\n",
       " '‚Äî': 33,\n",
       " '‚Äò': 34,\n",
       " ':': 35,\n",
       " '‡•ß': 36,\n",
       " '√®': 37,\n",
       " '‚àí': 38,\n",
       " '@': 39,\n",
       " 'C': 40,\n",
       " '‡•∞': 41,\n",
       " '‚Äï': 42,\n",
       " '‡§ø': 43,\n",
       " 'x': 44,\n",
       " 'D': 45,\n",
       " '‡§é': 46,\n",
       " '%': 47,\n",
       " '‚Äì': 48,\n",
       " '‡§ì': 49,\n",
       " '‡•Ö': 50,\n",
       " '√î': 51,\n",
       " '‚òº': 52,\n",
       " '4': 53,\n",
       " '‡§Ø': 54,\n",
       " '\\uf04a': 55,\n",
       " 'E': 56,\n",
       " '‡§´': 57,\n",
       " '‡§ñ': 58,\n",
       " '*': 59,\n",
       " 'r': 60,\n",
       " '¬≤': 61,\n",
       " '√á': 62,\n",
       " '‡•™': 63,\n",
       " '‡§†': 64,\n",
       " 'g': 65,\n",
       " '‚òØ': 66,\n",
       " '‡•Ç': 67,\n",
       " '/': 68,\n",
       " '‡•Å': 69,\n",
       " 'F': 70,\n",
       " '‡§É': 71,\n",
       " 'i': 72,\n",
       " '‡•®': 73,\n",
       " '‡§¢': 74,\n",
       " '5': 75,\n",
       " '\\u3000': 76,\n",
       " '‡§û': 77,\n",
       " '‡§º': 78,\n",
       " '‡§≤': 79,\n",
       " '‡•ù': 80,\n",
       " '\\u200c': 81,\n",
       " '‡•†': 82,\n",
       " '‡§¶': 83,\n",
       " '¬•': 84,\n",
       " '‡•á': 85,\n",
       " '‡§®': 86,\n",
       " '¬®': 87,\n",
       " 'üå∑': 88,\n",
       " '\\n': 89,\n",
       " '1': 90,\n",
       " 'R': 91,\n",
       " 'o': 92,\n",
       " '¬∞': 93,\n",
       " 'j': 94,\n",
       " '‡§ó': 95,\n",
       " 'c': 96,\n",
       " '‡•¶': 97,\n",
       " '`': 98,\n",
       " '\\u200b': 99,\n",
       " '√ó': 100,\n",
       " '‡•É': 101,\n",
       " '‡§à': 102,\n",
       " 'U': 103,\n",
       " '¬´': 104,\n",
       " '-': 105,\n",
       " '‡§ë': 106,\n",
       " '‡§∞': 107,\n",
       " '+': 108,\n",
       " ' ': 109,\n",
       " '‚ÅÑ': 110,\n",
       " '>': 111,\n",
       " '‡§Ü': 112,\n",
       " ';': 113,\n",
       " 'K': 114,\n",
       " '‡•≠': 115,\n",
       " '√û': 116,\n",
       " '‚âà': 117,\n",
       " 'y': 118,\n",
       " 's': 119,\n",
       " 'T': 120,\n",
       " '‡§ã': 121,\n",
       " '‡§Ö': 122,\n",
       " '‡§â': 123,\n",
       " 'h': 124,\n",
       " 'v': 125,\n",
       " '‡•Ä': 126,\n",
       " '\\uf0b4': 127,\n",
       " '‡§π': 128,\n",
       " '‡§ò': 129,\n",
       " '‡•≤': 130,\n",
       " '‡•ä': 131,\n",
       " '‡§Æ': 132,\n",
       " '‡§Ω': 133,\n",
       " '‡•ç': 134,\n",
       " '‡•´': 135,\n",
       " '‡§™': 136,\n",
       " 'd': 137,\n",
       " '‡§µ': 138,\n",
       " 'S': 139,\n",
       " 'n': 140,\n",
       " '‡§î': 141,\n",
       " '√ì': 142,\n",
       " '‡§≥': 143,\n",
       " '‡§∑': 144,\n",
       " '‡•§': 145,\n",
       " 'l': 146,\n",
       " '‚Ä†': 147,\n",
       " '‡§∂': 148,\n",
       " '‡§ê': 149,\n",
       " '\\U0001e240': 150,\n",
       " '¬∫': 151,\n",
       " '9': 152,\n",
       " '‡§ù': 153,\n",
       " '—å': 154,\n",
       " '|': 155,\n",
       " 't': 156,\n",
       " 'e': 157,\n",
       " '‚Äú': 158,\n",
       " '‡•â': 159,\n",
       " 'w': 160,\n",
       " '‚Äù': 161,\n",
       " '6': 162,\n",
       " '‡§á': 163,\n",
       " 'B': 164,\n",
       " 'H': 165,\n",
       " '\\xa0': 166,\n",
       " '‡••': 167,\n",
       " '‡§ö': 168,\n",
       " '\\u2003': 169,\n",
       " 'b': 170,\n",
       " '!': 171,\n",
       " '0': 172,\n",
       " '‡•Ü': 173,\n",
       " 'f': 174,\n",
       " '\\xad': 175,\n",
       " '#': 176,\n",
       " '‡•©': 177,\n",
       " '\\u200a': 178,\n",
       " '\\ufeff': 179,\n",
       " '.': 180,\n",
       " 'M': 181,\n",
       " '‡§Å': 182,\n",
       " '‡•å': 183,\n",
       " '√∑': 184,\n",
       " 'O': 185,\n",
       " '‡§è': 186,\n",
       " '‚Äô': 187,\n",
       " '‡•Ø': 188,\n",
       " '‡§ô': 189,\n",
       " 'a': 190,\n",
       " '3': 191,\n",
       " '\\t': 192,\n",
       " 'N': 193,\n",
       " '‡§•': 194,\n",
       " 'œÄ': 195,\n",
       " '‡•à': 196,\n",
       " '‡§£': 197,\n",
       " '‡§≠': 198,\n",
       " 'L': 199,\n",
       " '‡§ú': 200,\n",
       " '\\uf073': 201,\n",
       " '8': 202,\n",
       " '‡§©': 203,\n",
       " '(': 204,\n",
       " '‡§∏': 205,\n",
       " 'k': 206,\n",
       " '‡§ß': 207,\n",
       " '?': 208,\n",
       " 'X': 209,\n",
       " '‡§ü': 210,\n",
       " '‡§±': 211,\n",
       " '¬∑': 212,\n",
       " ',': 213,\n",
       " '‡•õ': 214,\n",
       " 'p': 215,\n",
       " '‡£ø': 216,\n",
       " '[': 217,\n",
       " '‚Ä¶': 218,\n",
       " '¬¥': 219,\n",
       " ']': 220,\n",
       " '‡§æ': 221,\n",
       " '‡§Ç': 222,\n",
       " '‡§¨': 223,\n",
       " '‡•ê': 224}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([136, 134, 107,  26, 101,   9,  43,  26,  29, 109, 153, 221, 136,\n",
       "       210,  89, 109, 105, 109, 163,  86, 134,  83,  43, 107, 221, 109,\n",
       "       168, 221, 136, 221,  95, 221, 102, 222,  89, 128,  43, 200, 221,\n",
       "        85, 109, 132, 221,  86, 134,   2,  85,  89, 205, 223, 194, 221,\n",
       "        85,  26, 109,   2, 221,  85,  24,  85, 107, 109, 136, 196, 205,\n",
       "       221,  26, 221,  85, 109,  79, 221,  95,  43, 109,  83, 221, 196,\n",
       "        24,  43,  86, 134, 194, 134,  54, 221,  85,  89,  26, 107, 134,\n",
       "       132, 213, 109,  26,  69,  26, 107, 134, 132])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one hot encoder\n",
    "\n",
    "def one_hot_encoder(encoded_text, num_uni_characters):\n",
    "    \n",
    "    \n",
    "    # encoded_text --> batch of encoded text\n",
    "    # num_uni_characters --> len(set(text))\n",
    "    \n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_characters))\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_characters))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,2,0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape(4,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generate (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X:\n",
    "    \n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    \n",
    "    [[ 2 3 4]]\n",
    "    \n",
    "    encoded_text : Complete Encoded Text to make batches from\n",
    "    batch_size : Number of samples per batch\n",
    "    seq_len : Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
    "    # characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # Use int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # Cut off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, dropout_porbability=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_porbability = dropout_porbability\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:ind for ind, char in decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            len(self.all_chars),\n",
    "            num_hidden,\n",
    "            num_layers,\n",
    "            dropout=dropout_porbability,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_porbability)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "    \n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda()\n",
    "            )\n",
    "        else:\n",
    "            hidden = (\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden)\n",
    "            )\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    dropout_porbability=0.5,\n",
    "    use_gpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1849347"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205483"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "tracker = 0\n",
    "\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 3.4744036197662354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3a90af4e75e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# POSSIBLE EXPLODING GRADIENT PROBLEM!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nepali_poem.net'\n",
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(225, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=225, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    dropout_porbability=0.5,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "    \n",
    "    encoded_text = model.encoder[char]\n",
    "    \n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    \n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "    \n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    \n",
    "    lstm_out , hidden = model(inputs, hidden)\n",
    "    \n",
    "    probs = F.softmax(lstm_out, dim=1).data\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        probs = probs.cpu()\n",
    "        \n",
    "    probs, index_positions = probs.topk(k)\n",
    "    \n",
    "    index_positions = index_positions.numpy().squeeze()\n",
    "    \n",
    "    probs = probs.numpy().flatten()\n",
    "    \n",
    "    probs = probs/probs.sum()\n",
    "    \n",
    "    char = np.random.choice(index_positions, p=probs)\n",
    "    \n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed=\"The\", k=1):\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    output_chars.append(char)\n",
    "    \n",
    "    for i in range(size):\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        output_chars.append(char)\n",
    "    \n",
    "    \n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§Æ‡•á‡§∞‡•ã ‡§æ ‡§æ ‡•ç     ‡§æ    ‡§æ ‡§æ‡•ç  ‡§æ‡•ç‡•ç ‡§æ  ‡•ç‡§æ‡§æ     ‡•ç ‡§æ  ‡§æ ‡§æ‡•ç‡•ç    ‡•ç  ‡§æ ‡•ç‡•ç‡•ç  ‡§æ ‡•ç‡§æ‡§æ‡•ç ‡•ç‡§æ  ‡§æ‡§æ‡§æ  ‡§æ‡§æ ‡•ç‡§æ‡•ç‡§æ  ‡•ç   ‡§æ‡•ç‡•ç‡§æ‡•ç‡§æ‡•ç  ‡§æ‡§æ‡§æ ‡•ç‡•ç‡§æ‡§æ  ‡•ç  ‡§æ‡•ç ‡§æ     ‡•ç‡•ç ‡§æ   ‡§æ‡•ç  ‡§æ‡§æ    ‡•ç‡§æ‡•ç‡§æ‡§æ‡§æ‡§æ  ‡§æ‡§æ‡•ç   ‡§æ ‡§æ     ‡§æ‡•ç‡•ç‡§æ  ‡•ç ‡§æ‡§æ‡§æ‡•ç   ‡§æ ‡•ç         ‡•ç‡§æ  ‡§æ‡•ç‡•ç ‡•ç  ‡§æ‡§æ  ‡§æ‡§æ ‡•ç‡§æ ‡§æ ‡§æ‡§æ‡•ç ‡§æ  ‡§æ ‡§æ‡•ç‡•ç  ‡§æ ‡•ç‡§æ  ‡•ç‡§æ‡§æ ‡§æ‡•ç      ‡§æ    ‡§æ‡§æ ‡§æ ‡•ç‡§æ‡§æ‡§æ‡§æ  ‡•ç‡§æ‡§æ‡•ç‡§æ   ‡§æ‡§æ  ‡§æ ‡•ç‡§æ‡•ç‡§æ ‡§æ‡§æ‡§æ  ‡•ç‡§æ‡§æ  ‡•ç‡§æ ‡§æ ‡§æ‡•ç‡•ç  ‡•ç ‡§æ‡§æ‡§æ‡§æ ‡•ç‡•ç ‡•ç‡§æ    ‡§æ          ‡•ç ‡•ç‡•ç‡•ç ‡§æ‡§æ‡§æ ‡•ç‡§æ‡§æ  ‡§æ‡•ç ‡§æ   ‡•ç     ‡§æ‡§æ ‡•ç  ‡•ç ‡§æ‡§æ   ‡•ç‡•ç  ‡•ç‡•ç  ‡§æ‡§æ ‡•ç‡§æ‡§æ‡§æ‡•ç ‡•ç‡•ç   ‡•ç‡§æ ‡§æ‡§æ ‡§æ ‡•ç‡§æ‡§æ   ‡§æ‡•ç ‡§æ ‡§æ‡•ç  ‡•ç‡§æ ‡•ç‡§æ‡§æ‡§æ ‡•ç ‡•ç‡§æ‡•ç‡•ç ‡§æ   ‡•ç ‡§æ‡§æ‡•ç‡•ç‡•ç  ‡§æ ‡§æ‡§æ ‡•ç ‡§æ‡§æ ‡§æ‡•ç  ‡§æ‡§æ  ‡§æ  ‡§æ    ‡•ç   ‡•ç  ‡§æ‡§æ‡•ç‡§æ‡§æ   ‡•ç  ‡§æ    ‡§æ    ‡§æ‡§æ ‡§æ‡•ç   ‡•ç  ‡•ç ‡§æ    ‡•ç‡•ç‡§æ ‡§æ    ‡§æ‡§æ‡•ç  ‡•ç ‡•ç ‡§æ ‡§æ‡•ç ‡•ç    ‡§æ ‡§æ‡§æ ‡§æ‡§æ‡§æ‡•ç ‡•ç‡§æ‡§æ‡•ç ‡§æ ‡§æ  ‡•ç‡§æ  ‡•ç‡•ç‡§æ‡§æ‡§æ   ‡•ç‡§æ‡•ç ‡•ç‡§æ‡§æ  ‡§æ‡§æ  ‡§æ‡•ç ‡•ç‡§æ‡§æ‡§æ ‡§æ    ‡•ç‡§æ ‡§æ‡•ç ‡§æ  ‡•ç  ‡§æ  ‡§æ‡•ç‡§æ‡§æ‡§æ‡§æ‡•ç ‡§æ‡•ç ‡•ç  ‡§æ‡•ç   ‡§æ      ‡§æ‡•ç‡•ç‡§æ ‡§æ‡•ç  ‡•ç‡•ç‡•ç ‡§æ ‡§æ ‡§æ     ‡•ç‡§æ      ‡•ç‡•ç   ‡§æ‡•ç‡•ç  ‡•ç  ‡§æ  ‡•ç  ‡§æ ‡•ç ‡•ç ‡§æ ‡•ç‡•ç ‡•ç‡•ç         ‡§æ‡•ç  ‡§æ       ‡§æ‡§æ  ‡§æ‡•ç‡•ç ‡•ç  ‡§æ  ‡§æ  ‡•ç‡§æ‡•ç  ‡•ç  ‡•ç   ‡§æ‡§æ  ‡§æ ‡§æ‡§æ‡§æ     ‡•ç ‡•ç‡§æ‡§æ‡§æ‡§æ‡•ç  ‡•ç ‡§æ‡•ç‡•ç‡§æ  ‡•ç‡§æ       ‡•ç ‡§æ    ‡•ç‡§æ‡•ç‡•ç‡§æ     ‡•ç‡§æ‡§æ‡§æ‡•ç ‡§æ ‡•ç‡§æ‡§æ ‡•ç ‡§æ     ‡§æ ‡•ç‡§æ  ‡§æ‡•ç‡•ç    ‡§æ  ‡•ç  ‡§æ ‡•ç ‡•ç ‡§æ ‡•ç‡•ç‡§æ‡•ç    ‡•ç‡§æ    ‡§æ    ‡§æ  ‡•ç‡§æ ‡•ç‡§æ‡§æ‡§æ‡•ç    ‡•ç‡§æ ‡§æ  ‡§æ‡§æ‡§æ‡•ç‡•ç‡•ç ‡§æ   ‡•ç‡•ç  ‡•ç    ‡•ç  ‡•ç  ‡§æ‡•ç‡§æ‡•ç    ‡§æ  ‡•ç‡§æ‡§æ ‡•ç‡•ç‡•ç   ‡•ç‡§æ ‡§æ     ‡§æ‡§æ    ‡§æ‡•ç‡•ç‡§æ  ‡§æ‡§æ‡§æ‡•ç‡§æ  ‡§æ‡•ç  ‡•ç‡§æ‡•ç   ‡•ç‡•ç ‡•ç ‡•ç ‡•ç‡•ç‡•ç‡•ç‡•ç‡•ç‡§æ‡§æ ‡§æ ‡•ç‡•ç\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed = \"‡§Æ‡•á‡§∞‡•ã \", k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
